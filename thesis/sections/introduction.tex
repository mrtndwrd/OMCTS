\chapter{Introduction}
\label{sec:introduction}
% This might be useful for AAMAS or something, but not for gaming comferences
%
%In the history of AI algorithms, playing games has been one of the objectives
%for a long time. These algorithms typically maximize their score or win
%probability. Early AI algorithms focused on simple games like tic tac toe,
%later focus was shifted to chess and even later to Go. Nowadays, many
%algorithms are designed for solving computer games. For example a lot of
%strategy games offer computer controlled contestants. 

General video game playing is a challenging problem, and since many real-world
problems can be modelled as a game, AI algorithms that can play
complex games successfully are often highly effective problem solvers as well.
%Game-specific AIs work well, but it is a time consuming task to create an AI for every game.  
Therefore, recent research focusses on algorithms capable of
solving several games with differing types of objectives.  A very common general
game solving approach is to use a tree search in order to select the best action
in a game state. The tree search is repeated in every new state until the game
ends. A popular example of this is \emph{Monte Carlo Tree Search (MCTS)}, which
achieves reasonable results on many games.

However, since many games are too complex to plan far ahead in a reasonable
time frame, planning by tree search based methods often only considers short-term
score differences and does not incorporate long-term plans. Moreover, many
algorithms lack common video game knowledge and do not use any of the knowledge
gained from the previous games.

In contrast, when humans play a game we expect them to do assumptions about
its mechanics, e.g., pressing the left button often results in the player's
avatar moving to the left on the screen. Players can use these assumptions to
learn how to play the game more quickly. Furthermore, human players have an
abstraction layer over their action choices; instead of choosing one action at a
time they define a specific subgoal for themselves.  For example: when there is
a portal on screen, a human player is likely to try to find out what the portal
does by walking towards it. Walking towards the portal can then be seen as a
subgoal of playing the game.

%\todo{Introduction of options (abstract thinking level)}
%	\todo{Introduction of learning about options}
%	\todo{Options have never been used with MCTS, this combination is
%	revolutionary!}

In certain situations, it is clear how such a subgoal can be achieved and a
sequence of actions, or \emph{policy}, can be defined to achieve it. A policy to
achieve a subgoal is called an option. Thus, an option selects an action, given
a game state, that aims at satisfying its subgoal.  Options, in this context, are
game-independent. For example, an option that has reaching a specific location
in the game (for example a portal) as its objective, selects actions using a
path planning heuristic that will reach the goal location. In this paper a new
algorithm, called \emph{Option MCTS (O-MCTS)} is introduced that extends MCTS to
use options. Because O-MCTS chooses between options rather than actions when
playing a game, we expect it to be able to plan at a higher level of
abstraction. Over time, information can be learned about the type of options
that is more useful in a game, which can be used to focus exploration of the
search tree to promising options. This information can also be transferred in
order to increase performance on a next, possibly harder, level. 

The algorithm will be benchmarked on four sets of games from the General Video
Game AI Competition \cite{perez2014}, against the standard Monte Carlo Tree
Search algorithm that is provided by that competition.  Our results indicate
that the resulting algorithm outperforms traditional MCTS.

