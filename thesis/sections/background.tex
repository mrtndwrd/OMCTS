\chapter{Background}
\label{sec:background}

This chapter explains the most important concepts needed to understand the
algorithms that are proposed in this thesis. We first describe \emph{Markov
decision processes (MDPs)}, the type of problem that our algorithms have to
deale with. Then  MCTS the tree search algorithm that is commonly used on MDPs.
Subsequently, options will be explained, these simulate the idea of defining
subgoals and reaching them.  Then, the basics of Q-learning are described.
Finally the \emph{video game description language (VGDL)} is explained. This is
the protocol that is used in the GVGAI competition to generate many different
games that all have the same interaction with the game playing algorithms.

\section{Markov Decision Processes}
\label{subsec:mdps}
In this thesis, each game will be treated as an MDP. MDPs provide a mathematical
framework for use in decision making problems. An MDP is formally defined as a
tuple $\langle S, A, T, R \rangle$. $S$ denotes the set of states. An MDP is
fully observable, meaning that a state contains all the information of the
game's current condition: locations of monsters, the avatar, etc. $A$ is a
finite set of actions, the input an agent can deliver to the game. $T$ is a
transition function defined as $T : S \times A \times S \rightarrow
\left[0,1\right]$. It specifies the probabilities over the possible next states,
when taking an action in a state.  $R$ is a reward function defined as $R: S
\times A \times S \rightarrow \mathbb{R}$. In this case, when the game score
increases, the increase is viewed as the reward. Algorithms typically maximize
the cumulative reward, which is analogous to the score. An MDP by definition has
the \emph{Markov property}, which means that the conditional probability
distribution of future states depends only upon the present state. No
information from previous states is needed. Algorithms do not have access to $T$
and $R$ in the scope of this thesis.

For example, for the game Zelda, a state $s$ consists of the location, rotation
and speed of the avatar, the location of the monsters, the locations of walls
and the location of the key and portal that need to be found. $S$ is the set of
all possible states, so all possible combinations of these variables. The action
set $A$ consists of the actions \textsc{up}, \textsc{down}, \textsc{left},
\textsc{right} and \textsc{use}. The transition function $T$ defines the
transition from a state, given an action. This means that the transition defines
the change in location of the monsters and the avatar, if any of the sprites
disappear (like when the avatar picks up the key), etc. Note that, since the
transition function is not by definition deterministic, the resulting state from
an action $a$ in state $s$ is not always the same state. For example: because a
monster moves about randomly, it could have moved left or right in the new
state. The reward function describes the change in game score, given a state,
action and resulting next state, for example when the avatar kills a monster
with the action \textsc{use}, its score will increase with 1.

\section{Monte Carlo Tree Search}
\begin{figure}
	\centering
	\epsfig{file=includes/mcts-wide.eps, width=\textwidth}
	\caption{One Monte Carlo tree search iteration}
	\label{fig:mcts}
\end{figure}

\label{subsec:mcts}
Monte Carlo methods have their roots in statistical physics, where they have
been used to approximate intractable integrals. Abrahamson
\cite{abramson1990expected} demonstrated theoretically that this sampling method
might be useful for action selection in games as well.  In 2001, Monte Carlo
methods were effectively used for bridge \cite{ginsberg2001gib}. The real
success of MCTS started in 2006, when the tree search method and UCT formula
were introduced, yielding very good results in Computer Go
\cite{gelly2006modification}. Since 2006, the algorithm has been extended with
many variations is still being used for other computer games
\cite{browne2012survey}, including the GVGAI competition
\cite{perez2014knowledge}.

This section explains how it approximates the value of actions taken in a
specific state. A tree is built incrementally from the states and actions that
are visited in a game. Each node in the tree represents a state, each connection
in the tree represents an action taken in that state leading to a state, which
is represented by the next tree node.  The process, as explained in Figure
\ref{fig:mcts}, consists of four phases that are constantly repeated. It is
started with the current game state, which is represented by the root node of
the tree. The first action is chosen by an \emph{expansion strategy} and
subsequently simulated, resulting in a new game state, for which a new node is
created. A \emph{rollout} is done from the new node, which means that a
simulation is run from the new node applying random actions until a predefined
stop criterion is met or the game ends. Finally, the score difference resulting
from the rollout is backed up, to the root node, which means that the reward is
saved to the visited nodes. Then a new iteration starts. When all actions are
expanded in a node, we call that node \emph{fully expanded} and use a
\emph{selection strategy} to select a next node. When a node is selected that is
not fully expanded, the expansion strategy is used to create a new node, after
which a rollout takes place and the results are backed up thereafter.

	%(Term from pMCTS.pdf)
The selection strategy selects optimal actions in internal tree nodes, depending
on the values of the child nodes. An effective and very popular selection
strategy is the \emph{upper confidence tree (UCT)} \cite{kocsis2006bandit},
which balances the choice between poorly explored actions with a high
uncertainty about their value and actions that have been explored but have a
higher value. A child node $j$ is selected to maximize
\begin{equation}
	\label{eq:uct}
	UCT = 2C_p \sqrt{\frac{2 \ln n_s}{n_{s'}}}
\end{equation}
Where $n_s$ is the number of times the current node $s$ has been visited,
$n_{s'}$ is the number of times child $c$ has been visited and $C_p > 0$ is a
constant, often set to $\sqrt{2}$, that shifts priority from exploration to
exploitation.

The traditional expansion strategy is to explore each action at least once in
each node. After all actions have been expanded, the node applies the selection
strategy for further exploration. Some variants of MCTS reduce the branching
factor of the tree by only expanding the nodes selected by a special expansion
strategy. A specific example is the \emph{crazy stone} algorithm
\cite{coulom2007efficient}, which is an expansion strategy that was designed
specifically for Go. We will use an adaptation of this strategy in the algorithm
proposed in Section \ref{sec:learning}.  When using crazy stone, an action $i$
is selected with a probability proportional to $u_i$
\begin{equation}
	\label{eq:crazystone}
	u_i = \exp\left(K \frac{\mu_0 - \mu_i}{\sqrt{2\left(\sigma_0^2 +
\sigma_i^2\right)}}\right) + \epsilon_i
\end{equation}
Each action has an estimated value $\mu_i$ ordered in such a way that $\mu_0 >
\mu_1 > \ldots > \mu_N$, and a variance $\sigma_i^2$. $\epsilon_i$ prevents 
the probability of selecting a move to reach zero and its value is proportional to
the ordering of the expected values of the possible actions. K is a constant
that influences the exploration/exploitation trade off.
\begin{equation}
	\label{eq:epsilon}
	\epsilon_i = \frac{0.1 + 2^{-i} + a_i}{N}
\end{equation}
Where $a_i$ is 1 when an action is \emph{an atari move}, a go-specific
move that can otherwise easily be underestimated by MCTS, and otherwise 0.

After a rollout, the reward is backed up, which means that the estimated value
for every node that has been visited in this iteration is updated with the
reward of this simulation. Usually the estimated value of a node is the average
of all rewards backed up to that node.

\section{Q-learning}
\label{subsec:qlearning}
Q-learning is a relatively simple model free temporal difference learning method
that was proposed in 1992 \cite{watkins1992q}. The \emph{Q-value}, $Q(s, a)$, is
the discounted reward that can be achieved by applying action $a$ to state $s$
and following the optimal policy afterwards. By learning Q-values for every
action in every state, it can estimate an optimal policy. 

The general idea of Q-learning is to incrementally save the reward from the MDP,
in combination with the current Q-value function: the update function uses the
reward and the maximum of the Q-values of the next state: 
\begin{equation}
	\label{eq:qlearning}
	Q(s, a) \gets Q(s, a) + \alpha \left(r + \gamma \max_a Q(s', a) - Q(s, a)\right),
\end{equation}
where $r$ is the reward that is achieved by using action $a$ in state $s$,
leading to state $s'$. The algorithm has two parameters: $\gamma$ is the
discount factor. This indicates the importance of future states. $\alpha$ is the
learning rate, which determines how quickly the Q-values are updated. An
exploration policy selects the actions for each state.  The Q-table can be used
to find the optimal policy by, for each state $s$, selecting the action $a$ that
maximizes $Q(s, a)$. Because after each action only one state-action pair is
being updated, Q-learning can take a long time to converge, but it is guaranteed
to converge to the optimal policy, given that the exploration policy visits each
state-action pair an infinite number of times and, for stochastic problems, the
learning rate $\alpha$ decreases over time.

\section{Options}
\label{subsec:options}
For mimicking human game playing strategies like defining and solving subgoals
and subtasks, we use options \cite{sutton1999between, barto2003recent}. An
option is a predefined method of reaching a specific subgoal. Formally, it is a
triple $\langle I, \pi, \beta\rangle$ in which $I \subseteq S$ is an initiation
set, $\pi: S \times A \rightarrow [0, 1]$ is a policy and $\beta: S^+
\rightarrow[0,1]$ is a termination condition.

A policy $\pi$ defines the action that should be taken in a state. The
initiation set $I$ is a set of states in which the option can be started. When the
option starts, policy $\pi$ will be followed, until a state is reached that
satisfies a termination condition in $\beta$. Using options in an MDP removes
the Markov property for that process: the state information alone is no longer
enough to predict an agent's actions, since the actions are now not only
state-dependant, but dependant on what option the agent has chosen in the past
as well. According to \cite{sutton1999between}, we can now view the process as
a \emph{semi-Markov decision process (SMDP)} \cite{duff1995reinforcement},
since its actions are of variable length. For convenience, we will call the
original action set of the MDP $A$, and the set of options $O$.  Normal actions
can be treated as options as well.  An option for action $a \in A$ has a
initiation set $I = S$, the policy $\pi$ is taking action $a$ in all the
states.  The termination condition $\beta$ is that action $a$ should be
performed once.


\section{Video Game Description Language}
\label{subsec:vgdl}
This thesis will use a framework called the video game description
language \cite{schaul2013video}, in which games can very easily be defined.

To define a game in VGDL, two files should be created. Firstly, the game
description should be made, which defines for each type of object what its
character in the level description is, what it looks like in the game, how it
interacts with other objects and the world, and when it disappears from the
game. Secondly, a level description should be made, in which each character maps
to an object in the game, on the same grid location as the character has in the
file. By only defining these two files a wide spectrum of games can be created.

In this thesis, we will use a Java implementation made for the GVGAI competition,
which comes with many games. The algorithm proposed in this thesis will be
benchmarked on these games, using the rules of the GVGAI competition. 
