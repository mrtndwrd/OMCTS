\chapter{Discussion}
\label{sec:conclusion}

\section{Conclusion}
We can conclude that the Option MCTS algorithm almost always performs at least
as good as MCTS. The O-MCTS algorithm excels in games with both a small level
grid or a small amount of sprites and high complexity, such as \textit{zelda},
\textit{overload} and \textit{eggomania}.  Furthermore, O-MCTS can look further
ahead than most tree searching alternatives, resulting in a high performance on
games like \textit{camel race}, in which reinforcement is sparse. This confirms
our hypothesis that using options, O-MCTS can win more games than MCTS. The
algorithm performs worse than expected in games with a high amount of sprites,
since the size of the option set becomes so large that maintaining it takes a
lot of time, leaving too little time for tree building. Over all twenty-eight
games, O-MCTS wins more games than MCTS.

The results of OL-MCTS indicate that it is possible to learn about which options
work better, meaning that in the future it should be possible to completely
remove infeasible options that have low expected rewards from the option set. We
expect that this could reduce the computation time O-MCTS needs to construct and
check all the options. However, more work needs to be done in this area to
enable improvement.

\section{Discussion}
This section will first discuss the trade-offs between scoring, winning and
playing time are discussed. Then, the evaluation of the SMDP Q-learning
algorithm is discussed and lastly this section discusses why OL-MCTS does not
introduce a significant improvement upon O-MCTS.

% Trade off between score and winning
A difficulty in game solving research is the trade-off between scoring, playing
time and winning. Winning earlier is often better, although for some games
winning later increases the eventual score. For example, in the game
\textit{butterflies}, butterflies keep spawning indefinitely. Each butterfly
that is caught, increases the score.  When all the butterflies are gone, the
game is won. An algorithm that catches all the butterflies as quickly as
possible will have a lower score than an algorithm that catches them less
efficiently. The question to ask then is: which of these is better? 
Another trade-off between winning and scoring can, for example, be found in
games such as \textit{zelda}. The goal is to pick up the key and walk to the
portal. If the monsters are killed, this increases the eventual score. But
trying to kill a monster comes at a risk: the player has to come close to the
monster in order to use his sword on it. So increasing the score, comes with the
risk of dying, decreasing the win ratio. It is often unclear how to prioritize
one of these objectives over the other. 

A solution can be found in the area of multi-objective optimization.
Multi-objective algorithms do not optimize one objective, but search for optimal
solutions that maximize a combination of several objectives at once instead. An
example of a multi-objective algorithm that has been used for general video game
playing, is \emph{multi-objective Monte Carlo tree search (MO-MCTS)} \cite{wang2012multi,
perez2015multi}.

% Q-learning should get more time to load its table
For the comparisson in this thesis, both SMDP Q-learning and OL-MCTS had to
adhere the same rules of the GVGAI competition. As a result, SMDP Q-learning was
allowed one second of initialization time. However, SMDP Q-learning with options
was originally proposed without any constraints on the initialization time or
Q-table size \cite{sutton1999between}.  Experiments with a longer initialization
time could bear interesting resulds, because Q-learning could learn from more
games without getting disqualified and its performance might improve. 

% Smaller option set
The experiments that have been done in Chapter \ref{sec:experiments}, all use
the same option set. However, the addition of a bad option in the option set might add
overhead without improving the mean score of the algorithm on any of the games.
Bad options should be taken out of the option set, improving the performance of
O-MCTS by removing overhead.More experiments need to be done to find out the influence
of each of the options. 

The OL-MCTS algorithm was constructed to reduce the overhead introduced by the
O-MCTS algorithm, but offers little improvement on most games. The current
hypothesis is that because the algorithm does not completely exclude the poorly
performing options from the option set, little of the overhead is reduced.  If a
threshold is set to the option values below which the options are not
maintained, the overhead of the algorithm could be reduced. This could, however,
introduce overfitting: options might be excluded prematurely, e.g., the
\texttt{GoToPosition} option for the portal in \textit{zelda}, which is closed
initially, should not be discarded because after picking up the key it becomes
useful. 
% TODO: Find some reference about a dynamic option set?

A similar limitation of the OL-MCTS algorithm is that it does not learn the
conditions in which options are good: it only learns the mean and variance of an
option's return. Although the crazy stone expansion algorithm is inclined to try
options with a high variance, an algorithm that can learn the conditions in
which these options result in a score improvement would yield better results.

\section{Future Work}
This section discusses future research that can be conducted to improve O-MCTS
and OL-MCTS. We propose methods for getting a better understanding of the
algorithms. 

Firstly, more research should be done in the individual options. Currently the
effectiveness of most of the options relies on the A Star implementation.  This
implementation is too time consuming, leaving little computation time for tree
building. In future work, trials should be done with simpler and computationally
cheaper alternatives, such as Enforced Hill Climbing as proposed in
\cite{ross2014general}. Although that algorithm has the problem that the agent
can get stuck, its reduced computation time might make it better suited for our
goal than the A Star algorithm. Alternatively, the human-defined option set can
be replaced by creating goal-oriented MDPs similar to PUMA's \cite{he2010puma}
or by using MAXQ \cite{dietterich2000hierarchical}. Automatically generated
options have the advantage that they are generated for one game specifically,
whereas our option set was created to work on as many games as possible. Because
of the limitations posed by the GVGAI competition, more investigation should be
done into how goal-oriented MDPs can be created with limited action time.

% Remove options from the option set after n games have been won
Another method to decrease the time it would take to maintain the option set is
by removing options from the option set after several games have been won.
By requiring the algorithm to first win some of the games, it is possible to
know which options never contribute to a positive score change. These options
can be removed from the option set, reducing the time it takes to maintain it.

% No/periodical interruption
Early in the development process, many of the options resulted in the agent
getting killed because it was following a specific option for too long. For this
reason interruption was introduced, as described in Section
\ref{subsec:options}. Since then, the options have been improved to less often
lead to the avatar's death, but because interruption was embedded in the
algorithms it was never disabled.  Although it is thought that this contributes
to the performance of the algorithm, research needs to be done in the impact it
has on the algorithms.

% Combining calculation time of several actions, (like Purofvio)
If interruption is disabled, some actions do not require any extra computation
time, since no new option has to be selected. The computation time of these
actions can be used to contribute to building the tree for the next option. This
means that whereas normally a tree has to be built in the time for \emph{one}
action, with interruption disabled it can be built in the combined time of all
the actions of an option. Since this multiplies the available computation time,
it is expected to have a positive effect on the performance of O(L)-MCTS.

% PTSP (like Purofvio)
One of the algorithms described in Chapter \ref{sec:background} is Purofvio
\cite{powley2012monte}.  This algorithm is comparable to O-MCTS, but was only
tested on the physical traveling salesperson problem. In order to compare the
performance of O-MCTS to shis algorithm it should be run on the PTSP problem as
well.

% Test if there really is no preference for longer options
Powley et al. suggest that Purofvio always prefers longer options over shorter
ones. We hypothesize that O-MCTS does not have this problem, due to the
discounting that is done by the backup function.  To support this claim, a test
should be done by creating an option set with two options.  One that achieves a
subgoal and one that achieves the same goal with more actions. If O-MCTS prefers
the shorter option, we can conclude that our backup method indeed solves Purofvio's
problem.

% Multi-objective?
The complicated trade-off between winning, scoring and playing time has to be
balanced by O-MCTS. This problem can be solved by using a multi-objective
algorithm. By basing another version of O-MCTS on MO-MCTS \cite{wang2012multi}
it might be possible to balance these trade-offs.

% Other option value methods
Lastly, in order to improve the learning algorithms, some other improvements can be
investigated. Firstly, the backup method can be tweaked. In
\cite{coulom2007efficient}, instead of the mean value other values like the
maximum return are used in the backup phase. This has a positive effect on the
action (or in our case option) choices, resulting in a better performance.
Furthermore, the mean and standard deviation of option returns are now
calculated over all the games, without regarding how long ago this game was
played. This might lead to underrated options, for example with doors that
unlock under specific conditions (for example when the key is picked up in
\textit{zelda}).  Using a maximum return or discounting the option values might
have a different effect.

