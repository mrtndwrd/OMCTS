\chapter{Prior Work}
Before describing the eventual product of this thesis, the O-MCTS algorithm,
this chapter will describe the work that has been done in the process of
arriving at the eventual algorithms. 

\section{A Test Game}

\lstinputlisting[caption=prey.txt, label=lst:preytxt, float=t]{../examples/gridphysics/prey.txt}

\lstinputlisting[caption=prey\_lvl0.txt, label=lst:preylvl, float=t]{../examples/gridphysics/prey_lvl0.txt}

For testing the performance of the algorithms, we create a simple test game. The
game should be very simple and easy to win, but it should be possible to see any
improvement an algorithm could achieve as well. We chose the \emph{predator \&
prey} game, in which the player is a predator, that should catch its prey (an
NPC) by walking into it. We decided to have three types of prey, one that never
moves, one that moves every 10 turns and one that moves every 2 turns. This
section describes how the game is made in VGDL.

As explained in chapter \ref{subsec:vgdl}, the first file that is needed by the
VGDL is the game description, which describes the game sprites and how they
interact with each other. Our game description can be seen in listing
\ref{lst:preytxt}.


Lines 2 to 8 describe the available sprites. There are two sprites in the game,
which are both \texttt{movable}: the avatar (predator) and the monster (prey).
The avatar is of type \texttt{MovingAvatar}, which means that the player has
four possible actions (up, right, down, left). The prey has three
instantiations, all of the type \texttt{RandomNPC}, which is an NPC that moves
about in random directions: the \texttt{inactivePrey}, which only moves every
3000 steps (which is more than the timeout explained shortly, so it never
moves); the \texttt{slowPrey} which moves once every 10 steps, and the
\texttt{fastPrey} which moves once every 2 steps.  By default, the
\texttt{MovingAvatar} can move once in each time step.

Lines 10 to 14 describe the level mapping. These characters can be used in the
level description files, to show where the sprites spawn. 

In the interaction set, Line 17 means that if the prey walks into the avatar (or
vice versa) this will kill the prey, and the player will get a score increase of
one point. Line 18 dictates that no \texttt{movable} sprite can walk through
walls.

Lastly, in the termination set, line 21 shows that the player wins when there
are no more sprites of the type \texttt{prey}, and line 22 shows that the player
loses after 100 time steps.

Listing \ref{lst:preylvl} shows a very simple level description. This level is
surrounded with walls and contains one avatar and one slow prey. 

\section{Q-learning}
\todo{ CONTINUE HERE!!!}
Traditionally, options are used in combination with Q-learning
\cite{sutton1999between}. In order to be able to compare our algorithm to the
Q-learning approach, we implement a variant of \emph{SMDP Q-learning} for
VGDL, based on the description made by Sutton et al. in section 3.2. Generally,
SMDP Q-learning estimates the transition function and expected rewards for using
an option in a certain state, in order to find an optimal policy over the option
set $O$.

SMDP Q-learning estimates an approximate option-value function $Q$, which
contains the expected return $Q(s, o)$ of using an option $o \in O$ in state $s
\in S$. Updates for the $Q$ function are done after each option termination by
\begin{equation}
	Q(s, o) \gets Q(s, o) + \alpha \left[r + \gamma^k \max_{o' \in O_{s'}}Q(s',
	o') Q(s, o)\right],
\end{equation}
where $\gamma$ is the discount factor, $k$ denotes the number of time steps
between the start state $s$ and stop state $s'$ for option $o$, $r$ denotes
the cumulative and discounted reward over this time, and step size parameter
$\alpha$ is similar to the learning rate parameter from traditional Q-learning.

For our implementation, the state information that is available in the Java VGDL
framework was serialized in order to be able to distinguish between states. The
set of options $O$ was designed by us.
