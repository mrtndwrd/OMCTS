\chapter{Experiments}
\label{sec:experiments}
This chapter starts with a proof of concept, showing that the principle of
O-MCTS and OL-MCTS works on a very simple game. Consequently, we will run O-MCTS
and OL-MCTS on a set of twenty-eight different games from the VGDL framework and
compare them to MCTS's perfomance.  These games include all games from the first
four training sets of the GVGAI competition, excluding puzzle games that can be
solved by a simple exhaustive search and have no random component (like NPCs,
for example). Each game consists of 5 levels. Firstly, we will compare O-MCTS's
performance to that of MCTS, by showing the win ratio and mean score of both
algorithms on all the games.  Secondly we show the improvement that OL-MCTS can
make on the games if it is allowed 4 games of learning time, and show the
progress it makes over 5 games.  Lastly we compare the three algorithms by
summing over all the games.  The goal of the algorithms is to primarily win as
many games as possible, while secondly maximizing the score. 

For these experiments we construct an option set which is aimed at providing
action sequences for any type of game, since the aim here is general video game
playing. Note that a more specific set of options can be created when the
algorithm should be tailored to only one type of games and similarly, more
options can be added to the algorithm easily.

\begin{itemize}[noitemsep]
	\item \texttt{ActionOption} executes a specific action once and then
		stops.
	\item \texttt{AvoidNearestNpcOption} makes the agent avoid the nearest NPC
	\item \texttt{GoNearMovableOption} makes the agent walk towards a
		movable game sprite (defined as movable by the VGDL) and stops when it
		is within a certain range of the movable
	\item \texttt{GoToMovableOption} makes the agent walk towards a
		movable until its location is the same as that of the movable
	\item \texttt{GoToNearestSpriteOfType} makes the agent walk to the nearest sprite of
		a specific type
	\item \texttt{GoToPositionOption} makes the agent walk to a specific position
	\item \texttt{WaitAndShootOption} waits until an NPC is in a specific location and
		then uses its weapon.
\end{itemize}

For each option type, a subtype per visible sprite type is created during the
game. For each sprite, an option instance of its corresponding subtype is
created. For example, the game \textit{zelda}, as seen in Figure \ref{fig:zelda},
contains three different sprite types (excluding the avatar and walls);
monsters, a key and a portal. The first level contains three monsters, one key
and one portal, and the aim of the game is to collect the key and walk towards
the portal without walking into the monsters. The score is increased by 1 if a
monster is killed (i.e., its sprite is on the same location as the sword sprite)
if the key is picked up, and when the game is won. \texttt{GoToMovableOption} and
\texttt{GoNearMovableOptions} are created for each of the three monsters and
for the key. A \texttt{GoToPositionOption} is created for the portal.  One
\texttt{GoToNearestSpriteOfType} is created per sprite type. One
\texttt{WaitAndShootOption} is created for the monsters, and one
\texttt{AvoidNearestNpcOption} is created. This set of options is $O$ in
Algorithms \ref{alg:omcts} and \ref{alg:olmcts}. In a state where, for example,
all the monsters are dead, the possible option set $\mathbf{p}_s$ does not
contain the \texttt{AvoidNearestNpcOption} and \texttt{GoToMovableOption}s and
\texttt{GoNearMovableOption}s for the monsters.

The \texttt{GoTo\ldots} options all utilize an adaptation of the A Star
algorithm to plan routes. An adaptation is needed, because at the beginning of
the game there is no knowledge of which sprites are traversable and which are
not. Thus, during every move that is simulated by the agent, the A Star module
has to update its beliefs about the location of walls and other blocking
objects. This is accomplished by comparing the movement the avatar wanted to
make to the movement that was actually made in game. If the avatar did not move,
it is assumed that all the sprites on the location the avatar should have
arrived in are blocking sprites. A Star keeps a \emph{wall score} for each
sprite type. When a sprite blocks the avatar, its wall score is increased by
one. Additionally, when a sprite kills the avatar, its wall score is increased
by 100, in order to prevent the avatar from walking into killing sprites.
Traditionally the A Star's heuristic uses the distance between two points. Our A
Star adaptation adds the wall score of the goal location to this heuristic,
encouraging the algorithm to take paths with a lower wall score. This method
enables A Star to try to traverse paths that were unavailable earlier, while
preferring safe and easily traversable paths. For example in \textit{zelda}, a
door is closed until a key is picked up. Our A Star version will still be able
to plan a path to the door once the key is picked up, winning the game.

We empirically optimize the parameters of the O(L)-MCTS algorithm
for these experiments. We use discount factor $\gamma = 0.9$, maximum action
time $t = 40$ milliseconds. The maximum search depth $d$ is set to 70, which is
higher than most alternative tree search algorithms, for example in the GVGAI
competition, use. The number of node visits after which \textsf{uct} is used,
$v$, is set to 40. Crazy stone parameter $K$ is set to $0.5$.  For comparison,
we use the MCTS algorithm provided with the Java implementation of VGDL with its
default value of maximum search depth of 10. Both algorithms have \textsf{uct}
constant $C_p = \sqrt{2}$. Unfortunately, comparing to Q-learning with options
was impossible, because the state space of these games is too big for the
algorithm to learn any reasonable behavior. All the experiments are run on an
Intel i7 \todo{type} quad core processor with 6 GB of RAM memory. For all
experiments, each algorithm plays each of the 5 levels of each game
20 times. 

\section{A Test Game}

\lstinputlisting[caption=prey.txt, label=lst:preytxt, float=t]{../examples/gridphysics/prey.txt}

\lstinputlisting[caption=prey\_lvl0.txt, label=lst:preylvl, float=t]{../examples/gridphysics/prey_lvl0.txt}

For testing the performance of the algorithms, we create a simple test game. The
game should be very simple and easy to win, but it should be possible to see any
improvement an algorithm could achieve as well. We chose the \emph{predator \&
prey} game, in which the player is a predator, that should catch its prey (an
NPC) by walking into it. We decided to have three types of prey, one that never
moves, one that moves every 10 turns and one that moves every 2 turns. This
section describes how the game is made in VGDL.

As explained in chapter \ref{subsec:vgdl}, the first file that is needed by the
VGDL is the game description, which describes the game sprites and how they
interact with each other. Our game description can be seen in listing
\ref{lst:preytxt}.


Lines 2 to 8 describe the available sprites. There are two sprites in the game,
which are both \texttt{movable}: the avatar (predator) and the monster (prey).
The avatar is of type \texttt{MovingAvatar}, which means that the player has
four possible actions (up, right, down, left). The prey has three
instantiations, all of the type \texttt{RandomNPC}, which is an NPC that moves
about in random directions: the \texttt{inactivePrey}, which only moves every
3000 steps (which is more than the timeout explained shortly, so it never
moves); the \texttt{slowPrey} which moves once every 10 steps, and the
\texttt{fastPrey} which moves once every 2 steps.  By default, the
\texttt{MovingAvatar} can move once in each time step.

Lines 10 to 14 describe the level mapping. These characters can be used in the
level description files, to show where the sprites spawn. 

In the interaction set, Line 17 means that if the prey walks into the avatar (or
vice versa) this will kill the prey, and the player will get a score increase of
one point. Line 18 dictates that no \texttt{movable} sprite can walk through
walls.

Lastly, in the termination set, line 21 shows that the player wins when there
are no more sprites of the type \texttt{prey}, and line 22 shows that the player
loses after 100 time steps.

Listing \ref{lst:preylvl} shows a very simple level description. This level is
surrounded with walls and contains one avatar and one slow prey. 

\section{O-MCTS}
\label{subsec:omcts}
\begin{figure}
	\centering
	\makebox[\columnwidth]{\includegraphics[width=1.25\textwidth]{includes/wins}}
	\vspace{-.8cm}
	\caption{Win ratio of the algorithms per game on all levels.}
	\label{fig:wins}
	\centering
	\makebox[\columnwidth]{\includegraphics[width=1.25\textwidth]{includes/scores}}
	\vspace{-.8cm}
	\caption{Mean normalized score of the algorithms per game 1 means the
	highest score achieved by all the algoriths, 0 the lowest.}
	\label{fig:scores}
\end{figure}

In the first experiment, we compare O-MCTS and MCTS. Figures \ref{fig:wins} and
\ref{fig:scores} respectively show the win ratio and normalized score of the
algorithms for each game. The games are ordered by the performance an algorithm
that always chooses a random action, indicating the complexity of the games.
From left to right, the random algorithm's win ratio and score decreases. As can
be seen in Figure \ref{fig:wins}, the O-MCTS algorithms performs at least as
good as MCTS in almost all games. 

O-MCTS greatly outperforms MCTS in the games \textit{missile
command}, \textit{overload}, \textit{firestorms}, \textit{boulderchase},
\textit{zelda}, \textit{bait}, \textit{camel race}, \textit{eggomania},
\textit{firecaster}, \textit{chase} and \textit{lemmings}, winning many more
games. Analyzing the algorithm's actions for these games, we can see that
the algorithm succeeds in efficiently planning paths in a dangerous environment,
enabling it to do a further forward search than the ordinary Monte Carlo tree
search. \textit{Camel race} requires the player to move to the right for 80
consecutive turns, which is very hard for MCTS, since it only looks forward 10 
turns. O-MCTS almost always wins, since it can plan forward a lot further. In
Overload, a sword has to be picked up before the avatar can finish the game,
which seems to be too hard for MCTS, but poses less of a problem for O-MCTS.
Furthermore, in Zelda we can see that the MCTS algorithm achieves roughly the
same score as O-MCTS, but does not win the game, since picking up the key and
walking towards the door is a difficult action sequence. We assume that the
score achieved by MCTS is because it succeeds in killing the monsters, whereas
O-MCTS achieves its score by picking up the key and walking to the door.  These
results indicate that O-MCTS performs better than MCTS in games where a longer
sequence of action planning has to be done.

The MCTS algorithm performs significantly better than O-MCTS in
\textit{whackamole}, \textit{jaws}, \textit{seaquest} and \textit{plaque
attack}. The similarity of these games is that they have a very big number of
different sprites, for each of which several options have to be created by
O-MCTS.  When the number of options becomes too big, constructing the set of
possible options $\mathbf{p}_s$ for every state $s$ becomes so time-consuming
that the algorithm has too little time to build a tree and find the best
possible action. In another experiment, we increased the computation time for
O-MCTS to 120ms and found that the win ratio of O-MCTS increases to around $0.8$
for \textit{seaquest} and \textit{plaque attack}.
% TODO: I probably want to check if MCTS's performance also increases with more
% computation time

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{includes/winsOLMCTS}
	\vspace{-.8cm}
	\caption{Win ratio comparison of OL-MCTS and O-MCTS.}
	\label{fig:wins-olmcts}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{includes/scoresOLMCTS}
	\vspace{-.8cm}
	\caption{Normalized score comparison of OL-MCTS and O-MCTS.}
	\label{fig:scores-olmcts}
\end{figure}

\section{OL-MCTS}
\label{subsec:olmcts}
For the second experiment, we compare OL-MCTS to O-MCTS, by running it on the
same set of games. The option learning algorithm is allowed four learning games,
after which the fifth is used for the comparisons. Figures \ref{fig:wins-olmcts}
and \ref{fig:scores-olmcts} show the games for which the results of OL-MCTS
differ from O-MCTS. For the other games, the performance was approximately the
same. Here OL-MCTS1 shows the performance of OL-MCTS on the first game. OL-MCTS5
shows the performance of the algorithm after learning for four games. 

We can see that, although the first iteration of OL-MCTS sometimes performs a
bit worse than O-MCTS, the fifth iteration often scores at least as high, or
higher than O-MCTS. We expect that the loss of performance in OL-MCTS1 is
because of the overhead that is created by the crazy stone algorithm: A sorting
of all the options has to take place in each game state. The learning algorithm
significantly improves score and win ratio for the game \textit{bait}, which is
a game in which the objective is to reach a goal portal after collecting a key.
The player can push boxes around to open paths. There are holes in the ground
that kill the player, unless they are filled with boxes, which make both the
hole and the box disappear. Figure \ref{fig:learning-results} shows the
improvement in score and win ratio for this game. There are two likely 
explanations for this improvement: 1.) There are sprites that kill the player,
which are evaded by the algorithm when it has learned to do so.  2.) The
algorithm learns that it should pick up the key.

\begin{figure}
	\centering
	\subfigure[Learning \textit{bait}]{
		\includegraphics[scale=.44]{includes/learning}
		\label{fig:learning-results}
	}
	\subfigure[Totals]{
		\includegraphics[scale=.44]{includes/totals.pdf}
		\label{fig:total-results}
	}
	\caption{Learning improvement on game \textit{bait}, it shows win ratio and
		normalized score. Total number of wins of the algorithms on
	all games.}
\end{figure}

\section{Totals}
\label{subsec:totals}
Figure \ref{fig:total-results} shows the sum of wins over all games, all levels.
It shows a significant ($p < 0.05$) improvement of O-MCTS and OL-MCTS over MCTS.
There is no significant difference in performance of OL-MCTS over O-MCTS,
although our results indicate that it does improve for some of the games.

Summarizing, our tests indicate that on complex games O-MCTS outperforms MCTS.
For other games it performs at least as well, as long as the number of game
sprites is not too high. 
%TODO: How many sprites?
The OL-MCTS algorithm performs worse than O-MCTS before learning, on some of the
games, like \textit{surround}, \textit{plaqueattack}, and \textit{bait}, but
can, for some of the games, increase performance significantly.
