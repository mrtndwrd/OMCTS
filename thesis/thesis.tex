\documentclass[a4]{report}
\usepackage[dvips]{graphicx}
\usepackage[latin1]{inputenc}

\usepackage{hyperref}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[usenames]{xcolor}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{makebox}

\newcommand{\todo}[1]{{\bfseries\sffamily{\textcolor{red}{TODO:} #1}}}
\newcommand{\cn}{{\colorbox{red}{\color{white}{[citation needed]}}}}

\usepackage{illcmolthesis} % For title page, slightly modified

\title{Monte Carlo Tree Search with Options\\for General Video Game Playing}
\author{\href{mailto:mrtndwrd@gmail.com}{Maarten de Waard}}
\birthdate{November 14\textsuperscript{th}, 1989}
\birthplace{Alkmaar, the Netherlands}
\defensedate{February 22\textsuperscript{nd}, 2016}  
\supervisor{\href{mailto:SanderBakkes@gmailcom}{dr.\ ing.\ Sander Bakkes}}
\supervisor{\href{mailto:D.M.Roijers@uva.nl}{Diederik Roijers, MSc.}}
\committeemember{\href{mailto:M.W.VanSomeren@uva.nl}{dr.\ Maarten van Someren}}
\committeemember{\href{mailto:P.H.Rodenburg@uva.nl}{dr.\ Piet Rodenburg}}
%% TODO: UPDATE BEFORE HAND IN!
\degree{MSc in Artificial Intelligence}

\begin{document}
%\frontmatter

\lstset{ %
	frame=single,
	numbers=left,
	numberstyle=\color{gray},
}
\maketitle

\begin{abstract}
	General video game playing is a popular research area in artificial
	intelligence, since AI algorithms that successfully play many different
	games are often capable of solving other complex problems as well.  ``Monte
	Carlo Tree Search'' (MCTS) is a popular AI algorithm that has been used for
	general video game playing, as well as many other types of decision
	problems.  The MCTS algorithm always plans over actions and does not
	incorporate any high level planning, as one would expect from a human
	player. Furthermore, although many games have similar game dynamics, often
	no prior knowledge is available to general video game playing algorithms.
	In this thesis, we introduce a new algorithm called ``Option Monte Carlo
	Tree Search'' (O-MCTS). It offers general video game knowledge and high
	level planning in the form of options, which are action sequences aimed at
	achieving a specific subgoal. Additionally, we introduce ``Option Learning
	MCTS'' (OL-MCTS), which applies a progressive widening technique to the
	expected returns of options in order to focus exploration on fruitful parts
	of the search tree. Furthermore, we offer an implementation of ``SMDP
	Q-learning'' for general video game playing, which is the algorithm
	that is traditionally used in combination with options. Our algorithms
	are compared to MCTS and SMDP Q-learning on a diverse set of twenty-eight
	games from the GVGAI competition. Our results indicate that by using MCTS's
	efficient tree searching technique on options, O-MCTS performs better than
	SMDP Q-learning on all games. It outperforms MCTS on most of them,
	especially those in which a certain subgoal has to be reached before the
	game can be won.  Lastly, we show that OL-MCTS improves its performance on
	specific games by learning expected values for options and moving a bias to
	higher valued options.  \end{abstract}

\tableofcontents

\input{sections/introduction}

\input{sections/background}

\input{sections/related}

\input{sections/optionsGvgp}

\input{sections/planning}

\input{sections/learning}

\input{sections/experiments}

\input{sections/conclusion}
% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\bibliographystyle{abbrv}
\bibliography{bib}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************
\end{document}
