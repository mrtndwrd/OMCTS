\section{Introduction}
\label{sec:introduction}

%An increasingly important goal in decision theory is creating AI algorithms that
%are capable of solving more than one problem. A general AI, instead of a
%specific alternative, is considered a step towards creating a strong AI.  An
%especially challenging domain of AI is general video game playing.  Since many
%real-world problems can be modeled as a game, algorithms that can play complex
%games successfully are often highly effective problem solvers in other areas.
%Although decision theory and general video game playing are different research
%areas, both can benefit from each other.  An increase in performance in general
%video game playing can be found by using algorithms designed for complex
%decision-theoretic problems, whereas the algorithms that are created for general
%video game playing can be applied to other decision-theoretic problems.
%Furthermore, applying decision-theoretic algorithms to a new class of problems,
%like general video game playing, can lead to a better understanding of the
%algorithms, and new insights in their strengths and weaknesses. 

Recent game programming research focusses on algorithms capable of solving
several games with different types of objectives. A common approach is to use a
tree search in order to select the best action for any given game state. In
every new game state, the tree search is restarted until the game ends.  A
popular example is \emph{Monte Carlo tree search (MCTS)}.

A method to test the performance of a general video game playing algorithm is
by using the framework of the \emph{general video game AI (GVGAI)} competition
\cite{perez2014}. In this competition, algorithm designers can test their
algorithms on a set of diverse games. When submitted to the competition, the
algorithms are applied to an unknown set of games in the same framework to test
their general applicability. Many of the algorithms submitted to this contest
rely on a tree search method.

A limitation in tree search algorithms is that since many games are too complex
to plan far ahead in a limited time frame, many of these algorithms incorporate
a maximum search depth. As a result, tree search based methods often only
consider short-term score differences and do not incorporate long-term plans.
Moreover, many algorithms lack common video game knowledge and do not use any of
the knowledge gained from the previous games.

In contrast, when humans play a game we expect them to make assumptions about
its mechanics, e.g., pressing the left button often results in the player's
avatar moving to the left on the screen.  Furthermore, we expect human players
to define specific subgoals for themselves, e.g., when there is a portal on
screen, a player is likely to try to find out what the portal does by walking
towards it.  The player will remember the effect of this and use that
information for the rest of the game.

In certain situations it is clear how such a subgoal can be achieved and a
\emph{policy}, which defines which actions to take in which state, can be
defined to achieve it. A policy to achieve a specific subgoal is called an
\emph{option}~\cite{sutton1999between}.  Thus, an option selects an action,
given a game state, that aims at satisfying its subgoal. In this paper, options
are game-independent. The options are expected to guide the exploration of a
game's search space to feasible areas.

%In contrast to an algorithm with path planning options, the probability of an algorithm that does not use options to reach a specific game sprite is
%smaller, especially when the road to the subgoal does not indicate any advantage
%for the player. For instance, in the first few iterations of MCTS, the algorithm
%will be equally motivated to move 10 steps into the direction of a certain game
%sprite as it will be to do any other combination of 10 actions. 

We propose a new algorithm called \emph{option Monte Carlo tree search (O-MCTS)}
that extends MCTS to use options. Because O-MCTS chooses between options rather
than actions when playing a game, we expect it to be able to plan more
efficiently, at a higher level of abstraction. Furthermore, we introduce
\emph{option learning MCTS (OL-MCTS)}, an extension of O-MCTS that approximates
which of the available options work well for the game it is playing. This
can be used to shift the focus of the tree search exploration to more promising
options. This information can be transferred to subsequent levels in order to
increase performance.

We compare our algorithms to MCTS on games from the GVGAI competition. Our
results indicate that the O-MCTS and OL-MCTS algorithms outperform MCTS in games
that require a high level of action planning, e.g., games in which something has
to be picked up before a door can be opened. In most other games, O-MCTS and
OL-MCTS perform at least as well as MCTS\@.
