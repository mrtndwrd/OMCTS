\section{Introduction}
\label{sec:introduction}
%\todo{What is AI in games?}
%\todo{There exists game-specific AI, which works well}
In the history of AI algorithms, playing games has been one of the objectives
for a long time. These AI algorithms typically maximize their score or win
probability. Early AI algorithms focussed on simple games like tic tac toe, but
later, focus was shifted to chess and even later to Go. Nowadays, many
algorithms are designed for solving computer games. For example a lot of
strategy games offer computer controlled contestants. AI in computer games is
mostly designed with a specific game in mind. \todo{examples of good working
algorithms? Star craft competitions?} 
%	\todo{Next step: General AI}
%		\todo{e.g. GVGAI competition}
Game-specific AIs often work very well, but it is a very time consuming
task to create an AI for every possible game. More recent research focusses
on algorithms capable of solving several games with different types of
objectives. Algorithms for general game solving are competing against each other
in the \emph{general video game AI (GVGAI)} competition\cite{perez2014}. 

When humans start playing a game, they automatically do some assumptions about
the controls, e.g., pressing the `right' button should result in the player's
avatar moving to the right on the screen. Human players can do
assumptions about the game mechanics and use them to learn how to play the game
more quickly. Furthermore, humans players would intuitively have long-term
plans.  For example: when there's a portal on screen, a human player is more
likely try to find out what the portal does by walking into it, than to just try
random actions, which is what most AI algorithms do.

%\todo{Most effective algorithms on this moment are based on MCTS, but don't
%really use common knowledge, which is why we're doing this}
Many competitors in the GVGAI competition use an approach based on tree
search in order to select the best action to take in a state. This tree search
is repeated in every state until the game ends. Most of the algorithms lack
common video game knowledge and don't use any of the knowledge gained from the
previous games\footnote{in fact, the GVGAI competition currently prohibits algorithms to
transfer any information to the next game in its planning track, which is the
only track at the time of writing}. Furthermore, planning by tree search based
methods often only considers short-term score differences and do not include
long-term plans.
%\todo{When people play games, they use common knowledge about games to win}

%\todo{Introduction of options (abstract thinking level)}
%	\todo{Introduction of learning about options}
%	\todo{Options have never been used with MCTS, this combination is
%	revolutionary!}
In this paper a new algorithm is introduced that uses \emph{options}, which can
be seen as a form of prior knowledge and long-term planning. Options are
sequences of actions with a specific goal, for example walking to a portal or
avoiding an enemy. Over time, information can be learned about which options are
helpful and which are not. Using this information, exploration of
the search tree can be focussed on the options that seem promising, improving
the overall performance of the algorithm. This information can also be
transfered in order to increase performance on a next, possibly harder, level.
We propose combining options with \emph{Monte Carlo tree search (MCTS)},
resulting in a new algorithm that performs better on most games than ordinary
MCTS.

The next section will explain MCTS and options. The third section will then
explain the planning scenario of MCTS using options. The fourth will extend the
algorithm to a learning algorithm. Section five will show some experiments after
which section six will cover some related algorithms. The last section concludes
and summarizes the paper and offers some future improvement possibilities.
