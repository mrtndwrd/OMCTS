\section{Conclusions and Future Work}
\label{sec:conclusion}
We can conclude that the O-MCTS algorithm almost always performs at least
as good as MCTS\@. It excels in games with both a small level grid or a small
amount of sprites and high complexity, such as \textit{zelda}, \textit{overload}
and \textit{eggomania}.  Furthermore, O-MCTS can look further ahead than most
tree searching alternatives, resulting in a high performance on games like
\textit{camel race}, in which reinforcement is sparse. This confirms our
hypothesis that using options, O-MCTS can win more games than MCTS\@. The
algorithm performs worse than expected in games with a high amount of sprites,
since the size of the option set becomes so large that maintaining it takes a
lot of time, leaving too little time for tree building. Over all twenty-eight
games, O-MCTS wins more games than MCTS\@.

The results of OL-MCTS indicate that it is possible to learn about which options
work better, meaning that in the future it should be possible to completely
remove infeasible options that have low expected rewards from the option set. We
expect that this could reduce the computation time O-MCTS needs to construct and
check all the options. However, more work needs to be done in this area to
enable improvement.

Furthermore, more research should be done in the influence of the option set.
Currently the effectiveness of the algorithm strongly relies on the A Star
implementation. This implementation is too time consuming, leaving
little time for actual tree building. In future work, trials can be done with
simpler and computationally cheaper alternatives, such as for example Enforced
Hill Climbing, as proposed in \cite{ross2014general}, although that has the
problem that the agent can get stuck. Alternatively, by creating goal-oriented
MDPs similar to PUMA's, the algorithm could probably increase sturdiness.

In order to improve the learning algorithms, some other improvements can be
investigated. 
%Firstly, the backup method can be tweaked. In
%\cite{coulom2007efficient}, instead of the mean value other values like the
%maximum return are used in the backup phase. This has a positive effect on the
%action (or in our case option) choices, resulting in a better performance.
The mean and standard deviation of option returns are now calculated over all
the games, without regarding how long ago this game was played. This might lead
to underrated options, for example with doors that unlock under specific
conditions.  Using a maximum return or discounting the option values might have
a different effect.

