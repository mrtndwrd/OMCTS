\section{Conclusions and Future Work}
\label{sec:conclusion}
From the experimental results we may conclude that the O-MCTS algorithm almost
always performs at least as well as MCTS\@. It excels in games with both a small
level grid or a small amount of sprites and high complexity, such as
\textit{zelda}, \textit{overload} and \textit{eggomania}.  Furthermore, O-MCTS
can look further ahead than most tree searching alternatives, resulting in a
high performance on games like \textit{camel race}, in which reinforcement is
sparse. An inherent advantage of having deep search trees is that the
probability of an promising option not finishing reduces. We confirm our
hypothesis that by using options O-MCTS can win more games than MCTS\@. The
algorithm performs worse than expected in games with a high amount of sprites,
since the size of the option set becomes so large that maintaining it takes a
lot of time, leaving too little time for tree building.  Over all twenty-eight
games, O-MCTS wins more games than MCTS\@.

The results of OL-MCTS indicate that it is possible to learn about which options
work better, meaning that in the future it should be possible to completely
remove infeasible options that have low expected rewards from the option set. We
expect that this could reduce the computation time O-MCTS needs to construct and
check all the options. However, the algorithm can be further improved.

Furthermore, more research should be done in the influence of the option set.
The $A^*$ algorithm could be replaced by a simpler algorithm, such as Enforced
Hill Climbing~\cite{ross2014general}.
The learning algorithm could be improved by calculating the option values
differently. An alternative method can use discounting in order to prioritize
more recent observations.
