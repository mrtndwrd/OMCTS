\section{Background}
\label{sec:background}

%\todo{What should one understand in order to grasp this paper?}
This section discusses the two major components of the algorithm, Monte Carlo
Tree Search and Options. 

\subsection{Markov Decision Process}
%\todo{MDPs}
We will describe games as a \emph{Markov decision process (MDP)}, which is a
tuple $\langle S, A, T, R \rangle$ where $S$ is a finite set of states, $A$ a
finite set of actions, $T$ a transition function defined as $T : S \times A
\times S \rightarrow \left[0,1\right]$ and $R$ a reward function defined as $R:
S \times A \times S -> \mathbb{R}$. An MDP by definition has the \emph{Markov
property}, which means that the conditional probability distribution of future
states depends only upon the present state. No information on previous states
is needed.

\subsection{Monte Carlo Tree Search}
%\todo{MCTS}
Many of the algorithms submitted to the GVGAI competition are based on the MCTS
algorithm\cn, which approximates the value of actions taken in a specific state.
The basis of the process is as follows. A tree is built incrementally from the
states and actions that are visited in a game.  Each node in the tree represents
a state, each connection in the tree represents an action taken in that state
leading to the state represented by the next node.  From the root node, the tree
is traversed using a selection strategy, which selects the next action according
to the expected values of the child nodes. An expansion strategy chooses a new
action when the algorithm arrives at a leaf node, subsequently creating a new
node for the next state. The simplest expansion strategy uses every action once
from a not fully expanded node. After expanding a node, a \emph{rollout} is
done, by applying random actions until a certain stop criterion is met, or the
game ends. Then the results are \emph{backed up} to all visited nodes and a new
search starts from the root node.

	%\todo{Selection strategy}
	%(Term from pMCTS.pdf)
The selection strategy selects optimal actions in internal tree nodes, depending
on the values of the child nodes. An effective and very popular selection
strategy is the \emph{upper confidence tree (UCT)}\cite{kocsis2006bandit}, which balances the choice 
between poorly explored actions with a high uncertainty about their value, and
actions that have been explored but have a higher value. A child node $j$ is
selected to maximise
$$UCT = \bar{X}_j + 2C_p \sqrt{\frac{2 \ln n}{n_j}}$$
Where $n$ is the number of times the current node has been visited, $n_j$ is the
number of times child $j$ has been visited and $C_p > 0$ is a constant that
shifts priority from exploration to exploitation, which is often set to $\sqrt{2}$.
	
	%\todo{Expansion strategy}
In standard MCTS, each action is explored at least once in each node. After all
actions have been expanded, the node switches to the normal selection strategy
for exploration. Some variants reduce the branching factor of the tree by only
expanding the nodes selected by a special expansion strategy. An example is the
\emph{crazy stone} algorithm\cite{coulom2007efficient} in which an action $i$ is selected with a
probability proportional to $u_i$
$$u_i = \exp\left(-2.4\frac{\mu_0 - \mu_i}{\sqrt{2\left(\sigma_0^2 +
\sigma_i^2\right)}}\right) + \epsilon_i$$
Each move has an estimated value $\mu_i$ ordered in such a way that $\mu_0 >
\mu_1 > ... > \mu_N$, and a variance $\sigma_i^2$. $\epsilon_i$ prevents that
the probability of selecting a move goes to zero. Its value is proportional to
the ordering of the expected values of the possible actions.
$$\epsilon_i = \frac{0.1 + 2^{-i} + a_i}{N}$$
Where $a_i$ is 1 when an action is \emph{an atari move}, which is a go-specific
move that can easily be underestimated by MCTS.

	%\todo{Backup algorithm}
% TODO: Check if result and reward are used wrongly
After a rollout, the reward is backed up, which means that the estimated value
for every node that has been visited in this simulation is updated with the
reward of this simulation. Usually the estimated value of a node is the average
of all rewards backed up to that node.

\subsection{Options}
%\todo{Options}
An option, in relation to an MDP, is a triple $\langle I, \pi, \beta\rangle$ in
which $I \subseteq S$ is an initiation set, $\pi: S \times A \rightarrow [0, 1]$
is a policy and $\beta: S^+ \rightarrow[0,1]$ is a termination condition.\cn
(sutton and barto?)

A policy $\pi$ defines the action that should be taken in a state. The
initiation set is a set of states in which the option can be started. When the
option starts, policy $\pi$ will be followed, until a state is reached that
satisfies a termination condition in $\beta$. Using options in an MDP removes
the Markov property for that process: the state information alone is no longer
enough to predict an agents actions, since the actions are now not only
state-dependant, also dependant on what option the agent has chosen in the past.
The process is now called a \emph{semi-Markov decision process (SMDP)}.

Normal actions can also be denoted as options. An option for action $a$ has a
initiation set $I = S$, the policy $\pi$ is taking action $a$ in all the states.
The termination condition is that action $a$ has been performed once.
