\section{Background}
\label{sec:background}

%\todo{What should one understand in order to grasp this paper?}
% Before we go into ... we must explain ...
This section starts with a small introduction into \emph{Markov
Decision processes (MDPs)}. Then it discusses the two major components of the
algorithm, MCTS and options. It ends with a description of the \emph{video game
description language (VGDL)} framework

\subsection{Markov Decision Processes}
\label{subsec:mdps}
%\todo{MDPs}
In this paper, a game will be treated as an MDP, which is a
tuple $\langle S, A, T, R \rangle$ where $S$ is a finite set of states, $A$ a
finite set of actions, $T$ a transition function defined as $T : S \times A
\times S \rightarrow \left[0,1\right]$ and $R$ a reward function defined as $R:
S \times A \times S -> \mathbb{R}$. An MDP by definition has the \emph{Markov
property}, which means that the conditional probability distribution of future
states depends only upon the present state. No information on previous states
is needed.
\todo{terugkoppeling naar games (wat is states, wat is actions, wat is
score/reward, hoe relateren winnen en scores aan elkaar)}

\subsection{Monte Carlo Tree Search}
\label{subsec:mcts}
%\todo{MCTS}
\todo{MCTS is al heel lang een core-algoritme en in GVGAI is het ook zo}
\todo{relateren aan game states (wat is expansion? Wat is een rollout,
uitleggen hoe je de game uitspeelt)}
\todo{spaties tussen citaties en woorden}
Many of the algorithms submitted to the GVGAI competition are based on the MCTS
algorithm\cn, which approximates the value of actions taken in a specific state.
The basis of the process is as follows. A tree is built incrementally from the
states and actions that are visited in a game.  Each node in the tree represents
a state, each connection in the tree represents an action taken in that state
leading to the state represented by the next node.  From the root node, the tree
is traversed using a \emph{selection strategy}, which selects the next action according
to the expected values of the child nodes. An \emph{expansion strategy} chooses
a new action when the algorithm arrives at a leaf or not fully-expanded node,
subsequently creating a new node for the next state. The simplest expansion
strategy uses every action once from a not fully expanded node. After expanding
a node, a \emph{rollout} is done, by applying random actions until a certain
stop criterion is met, or the game ends. Then the results are \emph{backed up}
to all visited nodes and a new search starts from the root node.

	%\todo{Selection strategy}
	%(Term from pMCTS.pdf)
The selection strategy selects optimal actions in internal tree nodes, depending
on the values of the child nodes. An effective and very popular selection
strategy is the \emph{upper confidence tree (UCT)}\cite{kocsis2006bandit}, which balances the choice 
between poorly explored actions with a high uncertainty about their value and
actions that have been explored but have a higher value. A child node $j$ is
selected to maximise
$$UCT = 2C_p \sqrt{\frac{2 \ln n}{n_j}}$$
Where $n$ is the number of times the current node has been visited, $n_j$ is the
number of times child $j$ has been visited and $C_p > 0$ is a constant , often
set to $\sqrt{2}$, that shifts priority from exploration to exploitation.
	
	%\todo{Expansion strategy}
\todo{Uitleggen dat het een specifiek voorbeeld is, maar we gaan het later
gebruiken}
\todo{uitleggen dat het speciaal voor go is gemaakt}
In standard MCTS, each action is explored at least once in each node. After all
actions have been expanded, the node switches to the normal selection strategy
for exploration. Some variants reduce the branching factor of the tree by only
expanding the nodes selected by a special expansion strategy. An example is the
\emph{crazy stone} algorithm\cite{coulom2007efficient} in which an action $i$ is selected with a
probability proportional to $u_i$
$$u_i = \exp\left(-2.4\frac{\mu_0 - \mu_i}{\sqrt{2\left(\sigma_0^2 +
\sigma_i^2\right)}}\right) + \epsilon_i$$
Each move has an estimated value $\mu_i$ ordered in such a way that $\mu_0 >
\mu_1 > ... > \mu_N$, and a variance $\sigma_i^2$. $\epsilon_i$ prevents 
the probability of selecting a move to reach zero. Its value is proportional to
the ordering of the expected values of the possible actions.
$$\epsilon_i = \frac{0.1 + 2^{-i} + a_i}{N}$$
Where $a_i$ is 1 when an action is \emph{an atari move}, which is a go-specific
move that can otherwise easily be underestimated by MCTS.

	%\todo{Backup algorithm}
% TODO: Check if result and reward are used wrongly
After a rollout, the reward is backed up, which means that the estimated value
for every node that has been visited in this simulation is updated with the
reward of this simulation. Usually the estimated value of a node is the average
of all rewards backed up to that node.

\subsection{Options}
\label{subsec:options}
%\todo{Options}
\todo{Zeggen dat dit een goede manier is om subgoals en subtasks te modelleren}
An option, in relation to an MDP, is a triple $\langle I, \pi, \beta\rangle$ in
which $I \subseteq S$ is an initiation set, $\pi: S \times A \rightarrow [0, 1]$
is a policy and $\beta: S^+ \rightarrow[0,1]$ is a termination
condition.\cite{sutton1999between}

A policy $\pi$ defines the action that should be taken in a state. The
initiation set is a set of states in which the option can be started. When the
option starts, policy $\pi$ will be followed, until a state is reached that
satisfies a termination condition in $\beta$. Using options in an MDP removes
the Markov property for that process: the state information alone is no longer
enough to predict an agents actions, since the actions are now not only
state-dependant, also dependant on what option the agent has chosen in the past.
The process is now called a \emph{semi-Markov decision process (SMDP)}. For
convenience, we will call the original action set of the MDP $A$, and the set of
options $O$.

Normal actions can be treated as options as well. An option for action $a$ has a
initiation set $I = S$, the policy $\pi$ is taking action $a$ in all the states.
The termination condition is that action $a$ has been performed once.

\subsection{Video Game Description Language}
\label{subsec:vgdl}
%\todo{Video Game Description Language}
This paper will use a framework called the video game description
language\cite{schaul2013video}, in which games can very easily be defined.

To define a game in VGDL, two files should be created. Firstly, the game
description should be made, which defines for each type of object what its
character in the level description is, what it looks like in the game, how it
interacts with other objects and the world, and when it disappears from the
game. Secondly, a level description should be made, in which each character maps
to an object in the game, on the same grid location as the character has in the
file. By only defining these two files a wide spectrum of games can be created.

The GVGAI competition uses a Java implementation of this framework and comes
with many games. The algorithm proposed in this paper will be benchmarked on
these games.

\todo{Dit gaan we gebruiken, maar in een andere setting}

