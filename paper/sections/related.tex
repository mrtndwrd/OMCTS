\section{Related Work}
\label{sec:related}
This chapter covers some popular alternative methods for general video game
playing and prior work on tree search with options. \emph{Deep Q networks (DQN)}
is a popular algorithm that trains a convolutional neural network for a
game~\cite{mnih2013playing}, \emph{Planning under Uncertainty with Macro-Actions
(PUMA)} is a forward search algorithm that uses extended actions in
\emph{partially observable MDPs (POMDPs)}~\cite{he2010puma}. \emph{Purofvio} is
an algorithm that combines MCTS with macro-actions that consist of repeating one
action several times~\cite{powley2012monte}.

DQN is a general video game playing algorithm that trains a convolutional neural
network that has the last four pixel frames of a game as input and tries to
predict the return of each action. A good policy can then be created by
selecting the action with the highest return. In this case it was not desirable
to implement DQN because of the limitations proposed by our testing framework.
The GVGAI competition framework currently works best for planning algorithms
that use the forward model to quickly find good policies. Learning over the
course of several games is difficult. In contrast, DQN typically trains on one
game for several days before a good policy is found and does not utilize the
forward model, but always applies actions directly to the game in order to
learn.

Another alternative is the PUMA algorithm, which applies forward search to
options (referred to as macro-actions) and works on POMDPs. PUMA automatically
generates goal-oriented MDPs for specific subgoals, the advantage of which is
that effective options can be created without requiring any prior knowledge of
the (PO)MDP\@. The disadvantage is that this takes a lot of computation time and
thus would not work in the GVGAI framework, where only 40 milliseconds of
computation time is allowed between actions. Furthermore PUMA has to find out the
optimal length per macro-action, our algorithm can use options of variable
length with starting an stopping conditions.

Another algorithm that uses MCTS with macro actions is called Purofvio.
Purofvio plans over macro-actions which, in this case, are defined as repeating
one action several times. No more complex options are defined. The paper notes
that their options must always be of the same size, because they found that
otherwise MCTS seems to favor options with a longer time span over shorter
options. Furthermore Purofvio is only created for the physical travelling
salesperson problem. Although Purofvio could also work on other games, we
decided to create a different algorithm, that is capable of using more complex
options.
