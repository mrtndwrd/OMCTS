\section{Related Work}
\label{sec:related}
This section covers some popular alternative methods for general video game
playing and prior work on tree search with options. 
%\emph{Deep Q networks (DQN)}
%is a popular algorithm that trains a convolutional neural network for a
%game~\cite{mnih2013playing}, \emph{Planning under Uncertainty with Macro-Actions
%(PUMA)} is a forward search algorithm that uses extended actions in
%\emph{partially observable MDPs (POMDPs)}~\cite{he2010puma}. \emph{Purofvio} is
%an algorithm that combines MCTS with macro-actions that consist of repeating one
%action several times~\cite{powley2012monte}.

\emph{Deep Q networks (DQN)}~\cite{mnih2013playing} is a general video game playing algorithm that
trains a convolutional neural network that has the last four pixel frames of a
game as input and tries to predict the return of each action. A good policy can
then be created by selecting the action with the highest return. In this case it
was not desirable to implement DQN because of the limitations proposed by our
testing framework.  The GVGAI competition framework currently works best for
planning algorithms that use the forward model to quickly find good policies.
Learning over the course of several games is difficult. In contrast, DQN
typically trains on one game for several days before a good policy is found and
does not utilize the forward model, but always applies actions directly to the
game in order to learn.

Another alternative is the algorithm \emph{Planning under uncertainty with
Macro-Actions (PUMA)}, which applies forward search to options and works on
\emph{Partially Observable MDPs (POMDPs)}~\cite{he2010puma}. PUMA automatically
generates goal-oriented MDPs for specific subgoals, the advantage of which is
that effective options can be created without requiring any prior knowledge of
the POMDP\@. The disadvantage is that this takes a lot of computation time and
thus would not work in the GVGAI framework, where only 40 milliseconds of
computation time is allowed between actions. Furthermore PUMA has to find out
the optimal length per macro-action, our algorithm can use options of variable
length with starting an stopping conditions.

Another algorithm that uses MCTS with macro-actions is called \emph{Purofvio}.
Purofvio plans over simple macro-actions which are defined as repeating one
action several times~\cite{powley2012monte}. No more complex options are
defined.  All the options are of exactly the same size. Purofvio is created
solely for the physical travelling salesperson problem. Although Purofvio could
also work on other games, we decided to create a different algorithm, that is
capable of using more complex options.
