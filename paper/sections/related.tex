\section{Related Work}
\label{sec:related}
Alternatives exist to learn how to play games generally. This section will
explain the difference between O-MCTS; \emph{deep Q networks (DQN)}
\cite{mnih2013playing}, a popular algorithm that trains a convolutional neural
network for a game; and \emph{Planning under Uncertainty with Macro-Actions
(PUMA)} \cite{he2010puma}, another tree search algorithm with extended actions.

Deep Q networks train a convolutional neural network that has the
last four frames of a game as input and tries to predict the Q-values of each
action. A good policy can then be created by selecting the action with the
highest Q-value. The main difference between O-MCTS and DQN is that O-MCTS has
to select an optimal action after 40 milliseconds, even when playing an unknown
game. DQN learns for several days, after which a good policy can be learned.
Furthermore, DQN does not use a forward model, but always applies actions
directly to the game in order to learn. 
Therefore, no accurate comparison can be made.

\todo{Puma (literature/puma.pdf)}
Another alternative is the PUMA algorithm, which applies forward search to
options (referred to as macro-actions) and works on \emph{partially observable}
MDPs. The options that are used by PUMA are generated by creating a goal-oriented
MDP for a specific subgoal. The advantage of this is that effective options can
be created without having to know anything about the (PO)MDP. The disadvantage
is that this takes a lot of computation time. Furthermore, the forward search
that is used by the algorithm creates a tree with a higher branching factor.
\todo{dit afmaken}
