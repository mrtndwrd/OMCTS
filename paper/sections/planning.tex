\section{Planning}
\label{sec:planning}

%\todo{hieronder: Dit is onze contributie, ``a novel algorithm'', ``key
%insight'', ``main objective''}
%\todo{Wat gaan we doen: herhalen main objective, teruggrijpen op background en
%introductie}

Planning over options is traditionally done by applying Q-learning to a set of
options instead of the set of actions \cite{sutton1999between}. Q-learning
learns the transition and reward for each state, but as game complexity
increases, the state space grows as well, increasing Q-learning's memory usage
and computation time immensely. In this section we introduce \emph{option Monte
Carlo tree search (O-MCTS)}, a novel algorithm that plans over options using
MCTS, enabling the use of options in complex MDPs.  The resulting algorithm
achieves higher scores on complex games that have several sub-goals than
traditional MCTS.

%\todo{How to combine options with MCTS - what problems (challenges) occur?}
In normal MCTS, an action is represented by a connection from a state to a next
state. An option spans over several actions, which influences the tree search
method. Furthermore, when many options are defined, the branching factor of the
algorithm increases significantly. This problem will be addressed in the next
section, where we will incorporate learning in the algorithm.

\begin{figure}
	\centering
	\epsfig{file=includes/omcts.eps, width=.5\columnwidth}
	\caption{The search tree constructed by O-MCTS. In each blue box, one option
	is followed. The arrows represent actions chosen by the option. An arrow
leading to a blue box is an action chosen by the option represented by that box.}
	\label{fig:omcts-tree}
\end{figure}

Traditionally, when a node is at depth $n$, we know that $n$ actions have been
chosen to arrive at that node and the time in that node is $t+n$. If nodes in
O-MCTS would represent options and connections would represent option selection,
this property would be lost, complicating the comparison of different nodes at
the same level in the tree. Therefore, we chose to keep the tree representation
the same: a node represents a state, a connection represents an action. We
introduce a change in the expansion and selection strategies, which select
options, rather than actions. When a node has an unfinished option, the next
node will be created using an action selected by that option. When a node
contains a finished option (meaning that the current state satisfies its
termination condition $\beta$), a new option can be chosen by the expansion or
selection strategy. The search tree now looks like Figure \ref{fig:omcts-tree},
in which each blue box represents one option.

Options are hand defined. We created a set of options most of which use the A
Star algorithm to navigate the agent towards a specific game sprite or location
\cn.

\begin{algorithm}
	\caption{$\mathsf{O-MCTS}(O, r, t, d)$}
	\label{alg:omcts}
	\begin{algorithmic}[1]
		\State $C_{s \in S} \gets \emptyset$ \Comment{$\mathbf{c}_s$ is the set of children nodes of $s$}
		\State $\mathbf{o} \gets \emptyset$ \Comment{$o_s$ will hold the option followed in $s$}
		\While {$time\_taken < t$} \label{alg:omcts:mainloop}
			\State $s \gets r$ \Comment{start from root node}
			\While {$\neg \mathsf{stop}(s, d)$} \label{alg:omcts:innerloop}
				\If{$s \in \beta(o_s)$} \label{alg:omcts:sp} \Comment{if option stops in state $s$}
					\State $\mathbf{p}_s \gets \cup_o (s \in I_{o \in O})$ \Comment{$\mathbf{p}_s$ is available options}
				\Else
					\State $\mathbf{p}_s \gets \{o_s\}$ \Comment{no new option can be selected}
				\EndIf \label{alg:omcts:ep}
				\State $\mathbf{m} \gets \cup_o (o_{s \in \mathbf{c}_s})$ \Comment{set $\mathbf{m}$ to expanded options}
				\If{$\mathbf{p}_s = \mathbf{m}$} \Comment{if all options are expanded}
					\State $s' \gets \max_{c \in \mathbf{c}_s} \mathsf{uct}(s, c)$ \label{alg:omcts:uct} 
						\Comment{child from eq. \ref{eq:uct}}
					\State $s \gets s'$ \label{alg:omcts:ss} 
						\Comment{continue loop with new node $s'$}
				\Else \label{alg:omcts:sexpand}
					\State $\omega \gets \mathsf{random\_element}(\mathbf{p}_s - \mathbf{m})$ 
					\State $a \gets \mathsf{get\_action}(\omega, s)$ 
					\State $s' \gets \mathsf{expand}(s, a)$ 
						\Comment{create child $s'$ using $a$}
						\State $\mathbf{c}_s \gets \mathbf{c}_s \cup \{s'\}$ \Comment{add $s'$ to $\mathbf{c}_s$}
					\State $o_{s'} \gets \omega$
					\State \textbf{break} \label{alg:omcts:break}
				\EndIf \label{alg:omcts:eexpand}
			\EndWhile
			\State $\delta \gets \mathsf{rollout}(s')$ \label{alg:omcts:rollout}
				\Comment{simulate until $\mathsf{stop}$}
			\State $\mathsf{back\_up}(s', \delta)$ \label{alg:omcts:backup}
				\Comment{save reward to parent nodes}
		\EndWhile
		\State \Return{$\mathsf{get\_action}(\max_{o \in c_r} \mathsf{value}(o), r)$}
	\end{algorithmic}
\end{algorithm}
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{includes/zelda}
	\caption{Visual representation of the game Zelda.}
	\label{fig:zelda}
\end{figure}


We describe O-MCTS in Algorithm \ref{alg:omcts}. It is invoked with a set of
options $O$, a root node $r$, a maximum runtime $t$ in milliseconds and a
maximum search depth $d$. Two variables are instantiated. $C_s$ is a set of
sets, containing the set of child nodes for each node and $\mathbf{o}$ contains
which option is followed for each node. The main loop starts at line
\ref{alg:omcts:mainloop}, which keeps the algorithm running until time runs out.
The inner loop runs until a node $s$ is reached that meets a stop criterion
defined by the function \textsf{stop}, or a node is expanded into a new node.
In lines \ref{alg:omcts:sp} until \ref{alg:omcts:ep}, $\mathbf{p}_s$ is set to
all options that are available in $s$. If an option has not finished,
$\mathbf{p}_s$ contains only the current option. Otherwise, it contains all the options $o$ that have state $s$ in their
initiation set $I_o$. For example, when the agent is playing Zelda (see figure
\ref{fig:zelda} and the current
state $s$ shows no NPCs on screen. If there is an option $o$ that goes to NPCs,
$I_o$ will not contain state $s$, because there are no NPCs on screen, rendering
$o$ useless in state $s$. $\mathbf{p}_s$ will thus not contain option $o$, since
$s$ is not in $o$'s initiation set $I_o$.

Then, the four phases of Figure \ref{fig:mcts} are implemented as follows.  If
the set $\mathbf{p}_s$ is the same as the set of options in the children of $s$,
$m$, i.e. all options have been explored at least once in node $s$, a new node
$s'$ is \emph{selected} by \textsf{uct} In line \ref{alg:omcts:ss}, $s$ is
instantiated with the new node $s'$, continuing the inner loop using this node.
If some options are unexplored in node $s$, it is \emph{expanded} with a random,
currently unexplored option by lines \ref{alg:omcts:sexpand} to
\ref{alg:omcts:eexpand}. After expansion or when the stop criterion is reached,
the tree selection loop is stopped and a \emph{rollout} is done, resulting in
score difference $\delta$. This score difference is \emph{backed up} to the
parent nodes of $s$ using the backup function, after which the tree traversal
restarts with the root node $r$.

A couple of functions are used by Algorithm \ref{alg:omcts}. The function
\textsf{stop} returns true when either the game ends in state $s$ or the maximum
depth is reached in $s$. The function \textsf{get\_action} lets option $o$
choose the best action for the state in node $s$, \textsf{apply\_action} applies
the action to state $s$ in the simulator, returning the resulting state.The
function \textsf{expand} creates a new child node for node $s$ with option $o$.
The \textsf{rollout} function chooses random actions until \textsf{stop} returns
true, after which the difference in score achieved by the rollout is returned.
The \textsf{back\_up} function traverses the tree through all parents of $s$,
updating their expected value.

When the time is up, the algorithm chooses an option from $c_r$ corresponding to
the child node with the highest expected value. This option chooses the best
action for the state in the root node and that action is returned. The agent
plays out this action, and runs the algorithm again in the next state it
receives, by creating a new root node $r$ for that state. 

Note that there is a difference between planning using the forward model and
actually acting on the game. During planning, O-MCTS executes options until they are
finished, whereas when a game action has to be chosen, O-MCTS always selects the
option that maximizes the future reward in that state, even if the option chosen
in the previous state was not finished in the current state. This way, when a
state is reached that was not anticipated upon during planning, the right option
will still be chosen.
