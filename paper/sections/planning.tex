\section{Planning}
\label{sec:planning}

\todo{hieronder: Dit is onze contributie, ``a novel algorithm'', ``key
insight'', ``main objective''}

\todo{Wat gaan we doen: herhalen main objective, teruggrijpen op background en
intraoductie}

\todo{How to combine options with MCTS - what problems (challenges) occur?}

\todo{Options zijn ``hand defined''}

\todo{Voorbeeld voor de set van options, voor selectie, etc}

\todo{Alles in context van games plaatsen}

This section will explain how MCTS has to be adjusted in order to be able to
plan over options instead of actions.
\todo{In deze sectie doen we alleen plannen, in de volgende gaan we ook nog
leren!}
\todo{meer ``we''}
\todo{niet ``should be'', maar ``is''}

\todo{Comments in algoritme}
\todo{Verwijzen naar de equation van uct in algoritme}
\todo{stop moet met een depth en max\_depth aangeroepen}
\todo{$\beta$ als functie}
\todo{return}
\begin{algorithm}
	\caption{$\mathsf{O-MCTS}(r, max\_time)$}
	\label{alg:omcts}
	\begin{algorithmic}[1]
		\While {$time\_taken < max\_time$} \label{alg:omcts:mainloop}
			\State $s \gets r$
			\While {$\neg \mathsf{stop}(s)$} \label{alg:omcts:innerloop}
				\If{$s \in \beta_o$} \label{alg:omcts:sp}
					\State $P_s \gets \cup_o ( s \in I_o)$
				\Else
					\State $P_s \gets \{o\}$ %\Comment{No new option can be selected}
				\EndIf \label{alg:omcts:ep}
				\If{$P_s = c_s$}
					\State $s' \gets \mathsf{uct}(s)$ \label{alg:omcts:uct}
				\Else \label{alg:omcts:sexpand}
					\State $o \gets \mathsf{random\_element}(P_s - c_s)$ 
					\State $c_s \gets c_s \cup \{o\}$
					\State $s' \gets \mathsf{expand}(s, o)$
					\State $c_{s'} \gets \emptyset$
					\State \textbf{break} \label{alg:omcts:break}
				\EndIf \label{alg:omcts:eexpand}
				\State $s \gets s'$ \label{alg:omcts:ss}
			\EndWhile
			\State $\delta \gets \mathsf{rollout}(s')$ \label{alg:omcts:rollout}
			\State $\mathsf{back\_up}(s', \delta)$ \label{alg:omcts:backup}
		\EndWhile
		\State \todo{return}
	\end{algorithmic}
\end{algorithm}

% TODO: Node en state worden nu door elkaar gebruikt...
The new algorithm, \emph{Option MCTS}, is described in algorithm
\ref{alg:omcts}. O-MCTS should be invoked with a root node $r$ and a maximum
number of milliseconds to run. The main loop starts at line
\ref{alg:omcts:mainloop}, which keeps the algorithm running as long as is
allowed. The inner loop runs until a node $s$ is reached that meets a stop
criterion defined by a function \textsf{stop}, or a node is expanded into a new
node. In lines \ref{alg:omcts:sp} until \ref{alg:omcts:ep} variable $P_s$ is set
to the set of options that can be chosen in node $s$. When an option hasn't
finished, $P_s$ only contains the currently followed option. Else, $P_s$ will
contain all options that are available in $s$, i.e. all the options $o$ that
have state $s$ in their initiation set $I_o$. 
\todo{Hier bijv. uitleggen wat dat betekent}
When the set $P_s$ is the same as
the set of options in the children of $s$, $c_s$, a new node $s'$ is selected by
\textsf{uct}. Else, in lines \ref{alg:omcts:sexpand} to \ref{alg:omcts:eexpand}
a node $s$ is expanded with a random, currently unexplored option. In line
\ref{alg:omcts:ss} $s$ is instantiated with the new node $s'$, continuing the
while loop using the new node. When a state with a stop criterion is reached, a
rollout is done, resulting in score difference $\delta$. This score difference
is backed up to $s$'s parent nodes using a backup function, after which the
algorithm restarts at the root node $r$.

%\todo{Is this needed?}
A couple of functions are used by algorithm \ref{alg:omcts}. The condition defined by
$\mathsf{stop}$ returns true when the game ends in state $s$. Usually
a maximum depth is defined as well in order to save calculation time. The
function $\mathsf{expand}$ creates a new child node for node $s$, using option
$o$. The $\mathsf{rollout}$ function chooses random actions until the stop
criterion defined by $\mathsf{stop}$ is reached, after which the difference in
score achieved by the rollout is returned. The $\mathsf{back\_up}$ function
traverses the tree through all parents of $s$, updating their expected value,
\todo{dit is raar:} often just the mean of all these values,  with the $\delta$. 

This algorithm will explore all options and focus on those that seem promising.
Each option will be tried at least once in each state where no option is already
chosen. Although this can be viewed as a thorough exploration strategy, when the
number of options increases, this strategy might become infeasible.

\todo{Hoe werkt het de volgende keer dat ik het aanroep?}
