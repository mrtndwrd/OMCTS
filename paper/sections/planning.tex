\section{Planning}
\label{sec:planning}

%\todo{hieronder: Dit is onze contributie, ``a novel algorithm'', ``key
%insight'', ``main objective''}
%\todo{Wat gaan we doen: herhalen main objective, teruggrijpen op background en
%introductie}

Planning over options is traditionally done by applying Q-learning on a set of
options instead of the set of actions \cite{sutton1999between}. In this section
we explain our novel algorithm, \emph{option Monte Carlo tree search (O-MCTS)}
that does planning over options using MCTS, enabling the use of options in more
complex MDPs. The resulting algorithm is better capable of solving complex games
that have several sub-goals than traditional MCTS.

%\todo{How to combine options with MCTS - what problems (challenges) occur?}
In normal MCTS, each action is represented by one connection from a state to a
next state. An option spans over several actions, which is one of the challenges
that was solved to make the O-MCTS algorithm. Furthermore, when a lot
of options are defined, the branching factor of the algorithm increases
significantly. This problem will be addressed in the next section, where we will
incorporate learning in the algorithm.

\begin{figure}
	\centering
	\epsfig{file=fly.eps}
	\caption{\todo{Real figure} The search tree constructed by O-MCTS}
	\label{fig:omcts-tree}
\end{figure}

In normal MCTS when a node is at depth $n$, we know that $n$ actions have been
chosen to get there and thus the time in that node is $t+n$. 
If a node in O-MCTS would represent an option and a connection would represent
selecting a new option, this property would be lost. Therefore, we chose to keep
the tree representation the same: a node represents a state, a connection
represents an action. The difference is in the expansion and selection
strategies, which choose which option will be followed in the next node. When an
option is chosen, the only possible next node is created with the option's
action. When an option is done (meaning that the current state satisfies its
termination condition), a new option can be chosen by the expansion or selection
strategy. The search tree now looks like figure \ref{fig:omcts-tree}.
Options are hand defined. We created a set of options that mainly use the A*
algorithm (\cn?) to navigate the agent to a specific game sprite or location.

%\todo{Options zijn ``hand defined''}
%\todo{Voorbeeld voor de set van options, voor selectie, etc}

\todo{meer ``we''}
\todo{niet ``should be'', maar ``is''}

\todo{Comments in algoritme}
\todo{Verwijzen naar de equation van uct in algoritme}
\todo{stop moet met een depth en max\_depth aangeroepen}
\todo{$\beta$ als functie}
\todo{return}
\begin{algorithm}
	\caption{$\mathsf{O-MCTS}(r, max\_time)$}
	\label{alg:omcts}
	\begin{algorithmic}[1]
		\While {$time\_taken < max\_time$} \label{alg:omcts:mainloop}
			\State $s \gets r$
			\While {$\neg \mathsf{stop}(s)$} \label{alg:omcts:innerloop}
				\If{$s \in \beta_o$} \label{alg:omcts:sp}
					\State $P_s \gets \cup_o ( s \in I_o)$
				\Else
					\State $P_s \gets \{o\}$ %\Comment{No new option can be selected}
				\EndIf \label{alg:omcts:ep}
				\If{$P_s = c_s$}
					\State $s' \gets \mathsf{uct}(s)$ \label{alg:omcts:uct}
				\Else \label{alg:omcts:sexpand}
					\State $o \gets \mathsf{random\_element}(P_s - c_s)$ 
					\State $c_s \gets c_s \cup \{o\}$
					\State $s' \gets \mathsf{expand}(s, o)$
					\State $c_{s'} \gets \emptyset$
					\State \textbf{break} \label{alg:omcts:break}
				\EndIf \label{alg:omcts:eexpand}
				\State $s \gets s'$ \label{alg:omcts:ss}
			\EndWhile
			\State $\delta \gets \mathsf{rollout}(s')$ \label{alg:omcts:rollout}
			\State $\mathsf{back\_up}(s', \delta)$ \label{alg:omcts:backup}
		\EndWhile
		\State \todo{return}
	\end{algorithmic}
\end{algorithm}

% TODO: Node en state worden nu door elkaar gebruikt...
The new algorithm, \emph{Option MCTS}, is described in algorithm
\ref{alg:omcts}. O-MCTS should be invoked with a root node $r$ and a maximum
number of milliseconds to run. The main loop starts at line
\ref{alg:omcts:mainloop}, which keeps the algorithm running as long as is
allowed. The inner loop runs until a node $s$ is reached that meets a stop
criterion defined by a function \textsf{stop}, or a node is expanded into a new
node. In lines \ref{alg:omcts:sp} until \ref{alg:omcts:ep} variable $P_s$ is set
to the set of options that can be chosen in node $s$. When an option has not
finished, $P_s$ only contains the currently followed option. Else, $P_s$ will
contain all options that are available in $s$, i.e. all the options $o$ that
have state $s$ in their initiation set $I_o$. 
\todo{Hier bijv. uitleggen wat dat betekent}
When the set $P_s$ is the same as
the set of options in the children of $s$, $c_s$, a new node $s'$ is selected by
\textsf{uct}. Else, in lines \ref{alg:omcts:sexpand} to \ref{alg:omcts:eexpand}
a node $s$ is expanded with a random, currently unexplored option. In line
\ref{alg:omcts:ss} $s$ is instantiated with the new node $s'$, continuing the
while loop using the new node. When a state with a stop criterion is reached, a
rollout is done, resulting in score difference $\delta$. This score difference
is backed up to $s$'s parent nodes using a backup function, after which the
algorithm restarts at the root node $r$.

%\todo{Is this needed?}
A couple of functions are used by algorithm \ref{alg:omcts}. The condition defined by
$\mathsf{stop}$ returns true when the game ends in state $s$. Usually
a maximum depth is defined as well in order to save calculation time. The
function $\mathsf{expand}$ creates a new child node for node $s$, using option
$o$. The $\mathsf{rollout}$ function chooses random actions until the stop
criterion defined by $\mathsf{stop}$ is reached, after which the difference in
score achieved by the rollout is returned. The $\mathsf{back\_up}$ function
traverses the tree through all parents of $s$, updating their expected value,
\todo{dit is raar:} often just the mean of all these values,  with the $\delta$. 

This algorithm will explore all options and focus on those that seem promising.
Each option will be tried at least once in each state where no option is already
chosen. Although this can be viewed as a thorough exploration strategy, when the
number of options increases, this strategy might become infeasible.

\todo{Hoe werkt het de volgende keer dat ik het aanroep?}
