\section{Learning} 
\label{sec:learning} 
Although we expect O-MCTS improves the tree search, as the number of options
increases, we expect the branching factor of O-MCTS to increase. When many
options are defined, exploring all the options for each node that uses a
finished option becomes infeasible. In this section, we will define \emph{option
values}, the expected mean and variance of an option, which can be used to
estimate which options are feasible and which are not. Exploration can then
focus on the feasible options, enabling an even deeper search tree than O-MCTS,
in the same amount of time. We expect that this increases performance,
especially in games where the set of possible options is large, or where only a
small set of options is needed in order to win.

The return $R_o$ for using option $o$ from timestep $t$ to $t+n$ is calculated
by adding the discounted rewards $r_t$ for all of the states visited by that
option \cite{sutton1999between}. $$R_o = r_{t} + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^n
r_{t+n},$$ where $\gamma \in [0, 1]$ is the discount rate parameter, which
influences the importance of rewards that lay further in the future: an option
with reward 1 at timestep $t$ will get a greater return than an option with the
same reward at timestep $t+1$.  

For the purpose of generalizing, we divide the set of options into \emph{types}.
For example, an option for going to a movable sprite has type
\texttt{GoToMovableOption}. An instance of this option exists for each movable
sprite in the game. \emph{Subtypes} are defined as well, a subtype is made for
each sprite type (i.e., each different looking sprite). The mean and variance of
the options' discounted rewards are saved and calculated per subtype. Each time
an option $o$ is finished, its subtype's values $\mu_o$ and $\sigma_o$ are
updated by respectively taking the mean and variance of all the returns of this
subtype. By saving values per subtype the algorithm can generalize over sprites
of the same type.

Using the expected mean and variance of the return $R_o$ of option $o$, we can
incorporate the crazy stone algorithm from Equation \ref{eq:crazystone} to shift
the focus of exploration to promising regions of the tree. The crazy stone
algorithm is applied in the expansion phase of O-MCTS. As a result, not all
children of a node will be expanded, but only the ones selected based on crazy
stone. When using crazy stone, we can select the same option several times, this
enables deeper exploration of promising subtrees, even during the expansion
phase. After a predefined number of visits $v$ to a node, the selection strategy
\textsf{uct} is followed in that node to tweak the option selection. When it
starts using \textsf{uct}, no new expansions will be done in this node.

The new algorithm can be seen in Algorithm \ref{alg:olmcts} and has two big
modifications. The updates of the option values are done in line
\ref{alg:olmcts:update}. The function \textsf{update\_values} takes the return
of the option $o$, and updates its mean $\mu_o$ and variance $\sigma_o$ by
calculating the new mean and variance of all returns of that option subtype. The
second modification starts on line \ref{alg:olmcts:ns}, where the algorithm
applies crazy stone if the current node has been visited less than $v$ times, or
alternatively applies \textsf{uct} similarly to O-MCTS. The
\textsf{crazy\_stone} function returns a set of weights over the set of possible
options $\mathbf{p}_s$. A simple weighted random then chooses a new option
$\omega$ by using these weights.  If $\omega$ has not been explored yet, i.e.,
there is no child node of $s$ in $c_s$ that uses this option, the algorithm
chooses and applies an action, and breaks to rollout in lines
\ref{alg:olmcts:scs} to \ref{alg:olmcts:ecs}. This is similar to the exploration
steps in O-MCTS. If $\omega$ has been explored in this node before, the
corresponding child node $s'$ is selected from $c_s$ and the loop continues
similar to when \textsf{uct} selects a child.

The information learned in a game can be transferred if the game is played
again by supplying OL-MCTS with the $\mu$ and $\sigma$ of the previous game. We
will refer to this as \emph{Option Transfer Learning MCTS (OTL-MCTS)}. Especially
in games where some options excel over the others, transfer learning can help
identifying these and focussing exploration on these options. 

We expect that by learning option values and applying crazy stone, the algorithm
can create even deeper search trees, focussed more on promising areas of the
search space, thereby improving its performance more than O-MCTS. Furthermore,
we expect that transferring option values to the next game, the algorithm can
improve over the course of several games.

\begin{algorithm}
	\caption{$\mathsf{OL-MCTS}(O, r, t, d, v, \mu, \sigma)$}
	\label{alg:olmcts}
	\begin{algorithmic}[1]
		\State $C_{s \in S} \gets \emptyset$
		\State $\mathbf{o} \gets \emptyset$
		\While {$time\_taken < t$} \label{alg:olmcts:mainloop}
			\State $s \gets r$
			\While {$\neg \mathsf{stop}(s, d)$} \label{alg:olmcts:innerloop}
				\If{$s \in \beta(o_s)$} \label{alg:olmcts:sp}
					\State $\mathsf{update\_values}(s, o_s, \mu, \sigma)$
						\Comment{Update $\mu$ and $\sigma$} \label{alg:olmcts:update}
					\State $\mathbf{p}_s \gets \cup_o (s \in I_{o \in O})$
				\Else
					\State $\mathbf{p}_s \gets \{o_s\}$
				\EndIf \label{alg:olmcts:scs}
				\State $\mathbf{m} \gets \cup_o (o_{s \in \mathbf{c}_s})$
				\If{$n_s < v$} \Comment{Apply \textsf{crazy\_stone}} \Comment{Eq. \ref{eq:crazystone}}
					\label{alg:olmcts:ns}
					\State $\mathbf{u}_s \gets \mathsf{crazy\_stone}(\mu, \sigma, \mathbf{p}_s)$
					\State $\omega \gets \mathsf{weighted\_random}(\mathbf{u}_s, \mathbf{p}_s)$
					\If{$\omega \not\in \mathbf{m}$} \Comment{option $\omega$ not expanded}
						\State $a \gets \mathsf{get\_action}(\omega, s)$ \label{alg:olmcts:scs}
						\State $s' \gets \mathsf{expand}(s, a)$ 
						\State $\mathbf{c}_s \gets \mathbf{c}_s \cup \{s'\}$
						\State $o_{s'} \gets \omega$
						\State \textbf{break} \label{alg:olmcts:ecs}
					\Else \Comment{Option $\omega$ already expanded}
						\State $s' \gets s \in \mathbf{c}_s : o_s = \omega$ \label{alg:olmcts:s}
					\EndIf
				\Else \Comment{Apply \textsf{uct}}
					\State $s' \gets \mathsf{uct}(s)$ \label{alg:olmcts:uct}
				\EndIf \label{alg:olmcts:ecs}
				\State $s \gets s'$ \label{alg:olmcts:ss}
			\EndWhile
			\State $\delta \gets \mathsf{rollout}(s')$ \label{alg:olmcts:rollout}
			\State $\mathsf{back\_up}(s', \delta)$ \label{alg:olmcts:backup}
		\EndWhile
	\end{algorithmic}
\end{algorithm}
